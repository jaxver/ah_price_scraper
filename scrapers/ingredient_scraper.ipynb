{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9368b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../meal_planning/Ingredients.xlsx')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame to avoid modifying the original\n",
    "scrape_df = df.copy()\n",
    "\n",
    "# Function to fetch product info with normal and bonus price\n",
    "def fetch_product_info(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "            return ('Not found', 'Not found', None, None, None, 'Not found', None, False)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Full name\n",
    "        name_span = soup.find(\"span\", class_=\"line-clamp_root__7DevG\")\n",
    "        fullname = name_span.text.strip() if name_span else 'Not found'\n",
    "        # Unit size\n",
    "        unit_span = soup.find(\"span\", class_=\"product-card-hero-price_unitSize__ReamD\")\n",
    "        unit = unit_span.text.strip() if unit_span else 'Not found'\n",
    "        # Current price (bonus if available, else normal)\n",
    "        price = None\n",
    "        bonus_price = None\n",
    "        normal_price = None\n",
    "        bonus_desc = None\n",
    "        in_bonus = False\n",
    "        # Bonus price\n",
    "        try:\n",
    "            bonus_div = soup.find(\"div\", class_=\"price-amount_root__Sa88q price-amount_bonus__zeyvy product-card-hero-price_now__zHsgu\")\n",
    "            if bonus_div:\n",
    "                integer = bonus_div.find(\"span\", class_=\"price-amount_integer__+e2XO\")\n",
    "                dot = bonus_div.find(\"span\", class_=\"price-amount_dot__MV39M\")\n",
    "                fractional = bonus_div.find(\"span\", class_=\"price-amount_fractional__kjJ7u\")\n",
    "                if integer and dot and fractional:\n",
    "                    bonus_price = float(f\"{integer.text}{dot.text}{fractional.text}\".replace(',', '.'))\n",
    "                    in_bonus = True\n",
    "        except Exception as e:\n",
    "            print(f\"Bonus price error for {url}: {e}\")\n",
    "            bonus_price = None\n",
    "        # Bonus sticker/description\n",
    "        try:\n",
    "            sticker_div = soup.find(\"div\", class_=\"promo-sticker_root__0ogs6 promo-sticker_bonus__UCZRY product-card-hero_promoSticker__iDm1j\")\n",
    "            if sticker_div:\n",
    "                bonus_desc = ' '.join(sticker_div.stripped_strings)\n",
    "                in_bonus = True\n",
    "        except Exception as e:\n",
    "            print(f\"Bonus sticker error for {url}: {e}\")\n",
    "            bonus_desc = None\n",
    "        # Normal price (was price)\n",
    "        try:\n",
    "            was_div = soup.find(\"div\", class_=\"product-card-hero-price_wasContainer__wBs7t\")\n",
    "            if was_div:\n",
    "                was_price_div = was_div.find(\"div\", class_=\"price-amount_root__Sa88q price-amount_was__ecJVB product-card-hero-price_was__4jz19\")\n",
    "                if was_price_div:\n",
    "                    integer = was_price_div.find(\"span\", class_=\"price-amount_integer__+e2XO\")\n",
    "                    dot = was_price_div.find(\"span\", class_=\"price-amount_dot__MV39M\")\n",
    "                    fractional = was_price_div.find(\"span\", class_=\"price-amount_fractional__kjJ7u\")\n",
    "                    if integer and dot and fractional:\n",
    "                        normal_price = float(f\"{integer.text}{dot.text}{fractional.text}\".replace(',', '.'))\n",
    "            # Fallback: if no was price, try hero price (non-bonus)\n",
    "            if normal_price is None:\n",
    "                hero_div = soup.find(\"div\", class_=\"price-amount_root__Sa88q product-card-hero-price_now__zHsgu\")\n",
    "                if hero_div and 'price-amount_bonus__zeyvy' not in hero_div.get('class', []):\n",
    "                    integer = hero_div.find(\"span\", class_=\"price-amount_integer__+e2XO\")\n",
    "                    dot = hero_div.find(\"span\", class_=\"price-amount_dot__MV39M\")\n",
    "                    fractional = hero_div.find(\"span\", class_=\"price-amount_fractional__kjJ7u\")\n",
    "                    if integer and dot and fractional:\n",
    "                        try:\n",
    "                            normal_price = float(f\"{integer.text}{dot.text}{fractional.text}\".replace(',', '.'))\n",
    "                        except Exception:\n",
    "                            normal_price = None\n",
    "        except Exception as e:\n",
    "            print(f\"Normal price error for {url}: {e}\")\n",
    "            normal_price = None\n",
    "        # Fallback: if no bonus price, use current price as normal\n",
    "        if bonus_price is not None:\n",
    "            price = bonus_price\n",
    "        elif normal_price is not None:\n",
    "            price = normal_price\n",
    "        else:\n",
    "            try:\n",
    "                price_div = soup.find(\"div\", class_=\"price-amount_root__Sa88q\")\n",
    "                if price_div:\n",
    "                    integer = price_div.find(\"span\", class_=\"price-amount_integer__+e2XO\")\n",
    "                    dot = price_div.find(\"span\", class_=\"price-amount_dot__MV39M\")\n",
    "                    fractional = price_div.find(\"span\", class_=\"price-amount_fractional__kjJ7u\")\n",
    "                    if integer and dot and fractional:\n",
    "                        price = float(f\"{integer.text}{dot.text}{fractional.text}\".replace(',', '.'))\n",
    "            except Exception as e:\n",
    "                print(f\"Fallback price error for {url}: {e}\")\n",
    "                price = None\n",
    "        date = datetime.today().date().isoformat()\n",
    "        # Smart label (e.g., 'vanaf')\n",
    "        smart_label = None\n",
    "        try:\n",
    "            smart_label_p = soup.find(\"p\", {\"data-testhook\": \"product-smart-label\"})\n",
    "            if smart_label_p:\n",
    "                smart_label = smart_label_p.get_text(strip=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Smart label error for {url}: {e}\")\n",
    "            smart_label = None\n",
    "        return (fullname, unit, price, normal_price, bonus_price, date, bonus_desc, in_bonus, smart_label)\n",
    "    except Exception as e:\n",
    "        print(f\"General error for {url}: {e}\")\n",
    "        return ('Error', 'Error', None, None, None, 'Error', None, False, None)\n",
    "\n",
    "urls = scrape_df['URL'].tolist()\n",
    "results = [None] * len(urls)\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    future_to_idx = {executor.submit(fetch_product_info, url): idx for idx, url in enumerate(urls)}\n",
    "    for future in as_completed(future_to_idx):\n",
    "        idx = future_to_idx[future]\n",
    "        results[idx] = future.result()\n",
    "\n",
    "valid_indices = []\n",
    "valid_results = []\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    if isinstance(r, (list, tuple)) and len(r) == 9:\n",
    "        valid_indices.append(i)\n",
    "        valid_results.append(r)\n",
    "    else:\n",
    "        print(f\"Skipping malformed result at index {i}: {r}\")\n",
    "\n",
    "results = [r for r in results if isinstance(r, (list, tuple)) and len(r) == 9]\n",
    "\n",
    "# Reduce the DataFrame to only the valid rows\n",
    "scrape_df = scrape_df.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "\n",
    "fullnames, units, latest_prices, normal_prices, bonus_prices, latest_dates, bonus_descs, in_bonuses, smart_labels = zip(*results)\n",
    "scrape_df['FullName'] = fullnames\n",
    "scrape_df['Unit'] = units\n",
    "scrape_df['Latest price'] = latest_prices\n",
    "scrape_df['Normal price'] = normal_prices\n",
    "scrape_df['Bonus price'] = bonus_prices\n",
    "scrape_df['Latest price date'] = latest_dates\n",
    "scrape_df['Bonus description'] = bonus_descs\n",
    "scrape_df['In bonus'] = in_bonuses\n",
    "scrape_df['Smart label'] = smart_labels\n",
    "scrape_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history for price correction if needed\n",
    "if os.path.exists('../meal_planning/Ingredients_history.xlsx'):\n",
    "    history_df = pd.read_excel('../meal_planning/Ingredients_history.xlsx')\n",
    "    def adjust_vanaf(row):\n",
    "        smart = str(row.get('Smart label', '')).lower()\n",
    "        if 'vanaf' in smart:\n",
    "            # Find last known normal price for this URL, most recent by Latest price date\n",
    "            prev = history_df[(history_df['URL'] == row['URL']) & (history_df['In bonus'] == False)]\n",
    "            prev = prev[prev['Normal price'].notnull() & (prev['Normal price'] != 'Not found') & (prev['Normal price'] != 'Error')]\n",
    "            if not prev.empty:\n",
    "                prev_sorted = prev.sort_values('Latest price date', ascending=False)\n",
    "                last_normal_price = prev_sorted.iloc[0]['Normal price']\n",
    "                return pd.Series({\n",
    "                    'Latest price': last_normal_price,\n",
    "                    'Bonus price': None,\n",
    "                    'In bonus': False,\n",
    "                    'Savings abs': None,\n",
    "                    'Savings %': None\n",
    "                })\n",
    "            else:\n",
    "                # No last known normal price, set blank\n",
    "                return pd.Series({\n",
    "                    'Latest price': None,\n",
    "                    'Bonus price': None,\n",
    "                    'In bonus': False,\n",
    "                    'Savings abs': None,\n",
    "                    'Savings %': None\n",
    "                })\n",
    "        return pd.Series({\n",
    "            'Latest price': row['Latest price'],\n",
    "            'Bonus price': row['Bonus price'],\n",
    "            'In bonus': row['In bonus'],\n",
    "            'Savings abs': row.get('Savings abs', None),\n",
    "            'Savings %': row.get('Savings %', None)\n",
    "        })\n",
    "    scrape_df[['Latest price', 'Bonus price', 'In bonus', 'Savings abs', 'Savings %']] = scrape_df.apply(adjust_vanaf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d35f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate savings columns\n",
    "def parse_bonus_price(row):\n",
    "    # Only parse bonus price if in bonus is True\n",
    "    if not row.get('In bonus', False):\n",
    "        return None\n",
    "    if row['Bonus price'] not in [None, 'Not found', 'Error'] and pd.notnull(row['Bonus price']):\n",
    "        return row['Bonus price']\n",
    "    desc = str(row.get('Bonus description', '')).lower()\n",
    "    normal = row['Normal price']\n",
    "    if normal in [None, 'Not found', 'Error'] or not pd.notnull(normal):\n",
    "        return None\n",
    "    # 1+1 gratis\n",
    "    if '1+1' in desc and 'gratis' in desc:\n",
    "        return round(normal / 2, 2)\n",
    "    # 2e halve prijs\n",
    "    if '2e halve prijs' in desc:\n",
    "        return round(normal * 0.75, 2)\n",
    "    # 2 voor' or '2 stuks voor'\n",
    "    match = re.search(r'2\\s*(?:stuks)?\\s*voor\\s*[Γé¼e]?\\s*([\\d,.]+)', desc)\n",
    "    if match:\n",
    "        try:\n",
    "            total = float(match.group(1).replace(',', '.'))\n",
    "            return round(total / 2, 2)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Apply bonus price logic\n",
    "scrape_df['Bonus price (parsed)'] = scrape_df.apply(parse_bonus_price, axis=1)\n",
    "# Use parsed bonus price if available, else original\n",
    "scrape_df['Bonus price final'] = scrape_df.apply(\n",
    "    lambda row: row['Bonus price (parsed)'] if row['Bonus price (parsed)'] not in [None, 'Not found', 'Error'] and pd.notnull(row['Bonus price (parsed)'])\n",
    "    else row['Bonus price'], axis=1)\n",
    "\n",
    "# Calculate savings columns using the final bonus price if present, else normal price\n",
    "scrape_df['Savings %'] = scrape_df.apply(\n",
    "    lambda row: round(((row['Normal price'] - row['Bonus price final']) / row['Normal price']), 2)\n",
    "    if row['In bonus'] and row['Normal price'] not in [None, 0, 'Not found', 'Error'] and row['Bonus price final'] not in [None, 0, 'Not found', 'Error'] and row['Normal price'] > 0\n",
    "    else None,\n",
    "    axis=1\n",
    ")\n",
    "scrape_df['Savings abs'] = scrape_df.apply(\n",
    "    lambda row: round((row['Normal price'] - row['Bonus price final']), 2)\n",
    "    if row['In bonus'] and row['Normal price'] not in [None, 'Not found', 'Error'] and row['Bonus price final'] not in [None, 'Not found', 'Error']\n",
    "    else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "scrape_df.drop(columns=['Bonus price (parsed)','Bonus price final'], inplace=True)\n",
    "\n",
    "print(scrape_df.head())\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_path = '../meal_planning/Ingredients.xlsx'\n",
    "scrape_df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- History tracking ---\n",
    "history_path = '../meal_planning/Ingredients_history.xlsx'\n",
    "today = datetime.now()\n",
    "scrape_df['Date'] = today.date().isoformat()\n",
    "scrape_df['WeekNr'] = today.isocalendar()[1]\n",
    "\n",
    "# Load history if exists, else create new\n",
    "if os.path.exists(history_path):\n",
    "    history_df = pd.read_excel(history_path)\n",
    "    # Drop any rows with the same date as the current scrape to avoid duplicates\n",
    "    history_df = history_df[history_df['Date'] != scrape_df['Date'].iloc[0]].copy()\n",
    "    # Find previous price for each ingredient (by URL or FullName)\n",
    "    last_scrape = history_df.groupby('URL').last().reset_index()\n",
    "    price_map = dict(zip(last_scrape['URL'], last_scrape['Latest price']))\n",
    "    def get_change(row):\n",
    "        prev = price_map.get(row['URL'])\n",
    "        if prev in [None, 'Not found', 'Error'] or row['Latest price'] in ['Not found', 'Error']:\n",
    "            return 'New' if prev is None else 'Error'\n",
    "        try:\n",
    "            prev_val = float(str(prev).replace(',', '.'))\n",
    "            curr_val = float(str(row['Latest price']).replace(',', '.'))\n",
    "            if curr_val > prev_val:\n",
    "                return 'Up'\n",
    "            elif curr_val < prev_val:\n",
    "                return 'Down'\n",
    "            else:\n",
    "                return 'Same'\n",
    "        except Exception:\n",
    "            return 'Error'\n",
    "    scrape_df['Change'] = scrape_df.apply(get_change, axis=1)\n",
    "    # Drop any all-NA columns from scrape_df before concatenation\n",
    "    scrape_df = scrape_df.dropna(axis=1, how='all')\n",
    "    history_df = pd.concat([history_df, scrape_df], ignore_index=True)\n",
    "else:\n",
    "    scrape_df['Change'] = 'New'\n",
    "    history_df = scrape_df.copy()\n",
    "\n",
    "history_df.to_excel(history_path, index=False)\n",
    "print('History updated:', history_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
